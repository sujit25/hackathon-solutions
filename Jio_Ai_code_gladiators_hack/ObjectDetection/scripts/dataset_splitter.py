from os import listdir, mkdir, getcwd
from os.path import join, exists
import csv
import shutil
from os.path import basename
import random


def maybe_create_dir(dir_name):
    '''creates directory if not exists already'''
    dir_abs = join(getcwd(), dir_name)
    if not exists(dir_abs):
        mkdir(dir_abs)


def read_csv_records(csv_fpath):
    '''
    read's csv records from a file
    :param csv_fpath: relative path of csv file
    :return: records of csv in list
    '''
    with open(csv_fpath, 'r', newline='') as full_csv:
        csv_reader = csv.reader(full_csv)
        data = []
        header= None
        for index, row in enumerate(csv_reader):
            if index == 0:
                header = row
            else:
                data.append(row)
        return header, data


def write_data_to_csv(csv_fpath, header, data):
    '''
    generates csv file
    :param csv_fpath: relative file path of csv
    :param header: header of csv
    :param data: data rows to be written to csv
    :return:
    '''

    with open(csv_fpath, 'w', newline='') as csv_file:
        csv_writer = csv.writer(csv_file)
        csv_writer.writerow(header)
        for row in data:
            csv_writer.writerow(row)


def generate_csv_from_data(header, csv_rows, dataset_dir, base_imgs_dir):
    '''
    generates train, val and test csv. Also copies images from source directory to dataset specific directory
    :param header: header of csv
    :param csv_rows: csv rows data specific to each dataset
    :param dataset_dir: it can be either "train", "val" or "test"
    :param base_imgs_dir: abs path of directory containing all images
    :return:
    '''

    filtered_imgs_names = [join(base_imgs_dir, csv_row[0]) for csv_row in csv_rows]
    #img_fpaths = [join(base_imgs_dir, img_name) for img_name in listdir(base_imgs_dir) if img_name in filtered_imgs_names]

    base_target_dir = join(getcwd(), dataset_dir)
    for img_fpath in filtered_imgs_names:
        target_fpath = join(base_target_dir, basename(img_fpath))
        shutil.copy(img_fpath, target_fpath)

    # generate csv after copying all files
    write_data_to_csv(dataset_dir + ".csv", header, csv_rows)


def split_dataset(images_dir, csv_fpath, val_split=0.1, test_split=0.1):
    '''
    :param images_dir: absolute path of directory containing all images
    :param csv_fpath: csv file generated by train_csv_generator
    :param val_split: split size for validation dataset
    :param test_split: split size for test dataset
    :return:
    '''

    header, csv_data = read_csv_records(csv_fpath)
    random.shuffle(csv_data)
    # create directories if not exists already
    maybe_create_dir("train")
    maybe_create_dir("val")
    maybe_create_dir("test")

    # split records in train, val and test
    train_split_index = len(csv_data) - int(len(csv_data) * (val_split + test_split))
    train_data = csv_data[: train_split_index]
    remaining_data= csv_data[ train_split_index:]

    val_split_index = len(remaining_data) - int(len(remaining_data)*test_split)
    val_data = remaining_data[:val_split_index]
    test_data = remaining_data[val_split_index:]

    # generate train, val and test csv. Also copy images to respective directory
    generate_csv_from_data(header, train_data, "train", images_dir)
    generate_csv_from_data(header, val_data, "val", images_dir)
    generate_csv_from_data(header, test_data, "test", images_dir)


if __name__ == '__main__':
    source_imgs_dir = "../images"
    csv_fpath = "../data.csv"
    split_dataset(source_imgs_dir, csv_fpath)
